{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## Double Deep Q-learning\n",
    "\n",
    "As before with the tabular Q-learning (part 1 of this tutorial), Double Deep Q-learning seeks to learn the \"quality\" function $Q(s,a)$. The difference in this case is that instead of using a table to store all the values of $Q(s,a)$, we are going to use a Neural Network to approximate the unknown function $Q(s,a)$.\n",
    "\n",
    "# Understanding Neural Networks\n",
    "\n",
    "Before we dive into the details of how Double Deep Q-learning works, let us first gain some intuition on how Neural Networks work.\n",
    "\n",
    "### A single neuron neural network\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jpmartinez10/RL-tutorial/master/numpy-based/imgs/nn_v1.png\">\n",
    "\n",
    "Neural networks can be seen as differentiable graphs where information can flow in two ways: forward way and backward way. For instance, consider the one neuron neural network shown in the figure. Here, the information flowing in the forward way is shown in color blue, and the information flowing in the backward way is shown in color red (the black arrows indicate the direction of the forward way).\n",
    "\n",
    "The forward way, or forward pass, is used to perform a series of math operations on the flowing information. This math operations are determined by the structure of the network (i.e. the connections, the activation functions, etc.) and the parameters of the network (i.e. weights and biases). On the other hand, the backward way, or backward pass, is used to distribute gradients of errors that will be used to update the parameters of the network, and hence, train the network to approximate a desired unknown function.\n",
    "\n",
    "The process of training a neural network can be summarized in the following steps:\n",
    "* 1. Feedforward an input of data (forward pass).\n",
    "* 2. Compute a loss between the network's output and the desired output.\n",
    "* 3. Backpropagate the gradient of the loss w.r.t each of the parameters of the network (backward pass).\n",
    "* 4. Use an optimization algorithm (i.e. gradient descent) to update the parameters.\n",
    "\n",
    "These steps are repeated until a stop criterion is met.\n",
    "\n",
    "Lets derive the expressions for a single iteration of these steps.\n",
    "\n",
    "##### Step 1: forward pass\n",
    "Assume the network receives an input of the form: $\\color{blue}{X} = [\\color{blue}{x_1},\\color{blue}{x_2},\\color{blue}{x_3}]^T$. Then, the feedforward equations are:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{blue}q = w_1\\color{blue}{x_1} + w_2\\color{blue}{x_2} + w_3\\color{blue}{x_3} + b = \\left[\\begin{array}{ccc}w_1&w_2&w_3\\end{array}\\right]\\left[\\color{blue}{\\begin{array}{c}x_1\\\\\n",
    "    x_2\\\\\n",
    "    x_3\\end{array}}\\right] + b = W\\color{blue}X + b\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{blue}z = f(\\color{blue}q) = f(W\\color{blue}X + b)\n",
    "\\end{equation}\n",
    "\n",
    "##### Step 2: compute the loss\n",
    "Asume that the desired output for the received input $\\color{blue}X$ is $\\hat{z}$, and assume that our loss function is the squared error. Then, the loss $L$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{2}\\big(\\hat{z}-\\color{blue}z\\big)^2\n",
    "\\end{equation}\n",
    "\n",
    "##### Step 3: backpropagate the gradient of the loss\n",
    "In this step we need to compute the gradient of the loss with respect to each of the (trainable) parameters of the network. To do so we use the backpropagation algorithm (this is a fancy name for the calculus's chain rule).\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_zL} = \\color{red}{\\frac{\\partial L}{\\partial z}} = -(\\hat{z}-\\color{blue}z)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_qL} = \\color{red}{\\frac{\\partial z}{\\partial q}}\\color{red}{\\frac{\\partial L}{\\partial z}} = f'(\\color{blue}q)\\color{red}{\\nabla_zL}\n",
    "\\end{equation}\n",
    "\n",
    "Apliying this same idea on the weight gates we find that:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{w_1}L} = \\color{blue}{x_1}\\color{red}{\\nabla_qL}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{w_2}L} = \\color{blue}{x_2}\\color{red}{\\nabla_qL}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{w_3}L} = \\color{blue}{x_3}\\color{red}{\\nabla_qL}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_bL} = \\color{red}{\\nabla_qL}\n",
    "\\end{equation}\n",
    "\n",
    "#### Step 4: update the weights\n",
    "In this simple example we will use vanilla gradient descent to update the weights. Here $\\alpha$ denotes the learning rate.\n",
    "\n",
    "\\begin{equation}\n",
    "    w_1 = w_1 - \\alpha\\color{red}{\\nabla_{w_1}L}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    w_2 = w_2 - \\alpha\\color{red}{\\nabla_{w_2}L}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    w_3 = w_3 - \\alpha\\color{red}{\\nabla_{w_3}L}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "These steps are repeated until a desired performance or stop criterion is achieved. \n",
    "\n",
    "It is important to notice that the gradient can be computed also w.r.t the inputs of the neuron. For instance:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{x_1}L} = w_1\\color{red}{\\nabla_qL}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{x_2}L} = w_2\\color{red}{\\nabla_qL}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{x_3}L} = w_3\\color{red}{\\nabla_qL}\n",
    "\\end{equation}\n",
    "\n",
    "## A multi-layer neural network\n",
    "In our single neuron example we assumed that the input of the neuron was a vector of data. However, the input of the neuron could also be the output of a previous layer in the network, and the gradients w.r.t the input would be the gradients backproagating to the previous layer. Once one understands this, it is straightforward to extrapolate the previous training procedure to networks with multiple layers and neurons.\n",
    "\n",
    "Consider the 2 layer neural network in the following figure:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jpmartinez10/RL-tutorial/master/numpy-based/imgs/nn_v2.png\">\n",
    "\n",
    "In this network each layer is a group of neurons and each of these neurons has its own set of weights and its own bias. Also, each neuron in one layer is connected to all the outputs of the neurons in the previous layer. For example, in the case of the layer 1, each neuron is connected to every input $(\\color{blue}X)$, and, in the case of layer 2, each neuron is connected to each output of the layer 1 $(\\color{blue}Y)$.\n",
    "\n",
    "\n",
    "The training procedure is exactly the same as with the one neuron neural network, the only change is in the size of the matrices.\n",
    "\n",
    "\n",
    "For instance consider receiving an input matrix $\\color{blue}X$ of size $d\\times m$. Here $d$ is the dimension of the input examples (i.e. in our one-neuron network example $d$ was $3$) and $m$ is the number of examples that we are going to feedforward to the network per training iteration (i.e. in our one-neuron network example $m$ was $1$). The term $m$ is known as the batch size and using $m>1$ usually allows faster and smoother training. Consider also a first layer with $n_1$ neurons and a second layer (output layer) with $n_2$ neurons. Under these asumptions the dimensions of the different matrices are:\n",
    "\n",
    "\\begin{equation}\n",
    "        \\color{blue}X \\rightarrow d\\times m\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "        W_1 \\rightarrow n_1\\times d\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "        B_1 \\rightarrow n_1\\times 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "        W_2 \\rightarrow n_2\\times n_1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "        B_2 \\rightarrow n_2\\times 1\n",
    "\\end{equation}\n",
    "\n",
    "##### Forward pass\n",
    "Assuming activation functions $f_1$ and $f_2$ for the layers 1 and 2 respectively, the forward pass equations are:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{blue}Y = f_1(W_1\\color{blue}X + B_1) \\rightarrow n_1 \\times m\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{blue}Z = f_2(W_2\\color{blue}Y + B_2) \\rightarrow n_2 \\times m\n",
    "\\end{equation}\n",
    "\n",
    "##### Compute the loss\n",
    "\n",
    "The loss function in this case is again defined as the Squared Error between the desired output and the actual output of the network. However, notice that in this case the error is divided by the batch size $m$ (thats an usual thing to do and it leads to the Mean Squared Error criterion). In this loss function all the operations are performed elementwise.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{2m}\\big(\\hat{Z}-\\color{blue}Z\\big)^2 \\rightarrow n_2 \\times m\n",
    "\\end{equation}\n",
    "\n",
    "##### Backward pass\n",
    "\n",
    "In this step we backpropagate the gradient of the loss.\n",
    "\n",
    "The gradient at the output of layer 2 is:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_ZL} = \\color{red}{\\frac{\\partial L}{\\partial Z}} = -\\frac{1}{m}(\\hat{Z}-\\color{blue}Z) \\rightarrow n_2 \\times m\n",
    "\\end{equation}\n",
    "\n",
    "As before, we can compute an intermediate gradient inside the neurons of layer 2 (this is done for efficiency as with the $\\color{blue}{q}$ term in the single neuron example). Here $*$ denotes elementwise multiplication.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{Q_2}L} =  f_2'(W_2\\color{blue}Y + B_2) * \\color{red}{\\nabla_ZL} \\rightarrow n_2 \\times m\n",
    "\\end{equation}\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{Y}L} =  W_2^T\\color{red}{\\nabla_{Q_2}L}  \\rightarrow n_1 \\times m\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{W_2}L} =  \\color{red}{\\nabla_{Q_2}L}\\color{blue}Y^T  \\rightarrow n_2 \\times n_1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{B_2}L} =  \\sum_{m}\\color{red}{\\nabla_{Q_2}L}  \\rightarrow n_2 \\times 1\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sum_{m}$ denotes summation over the $m$ columns (along the batch).\n",
    "\n",
    "Following the same ideas for layer 1 we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{Q_1}L} =  f_1'(W_1\\color{blue}X + B_1) * \\color{red}{\\nabla_YL} \\rightarrow n_1 \\times m\n",
    "\\end{equation}\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{X}L} =  W_1^T\\color{red}{\\nabla_{Q_1}L}  \\rightarrow d \\times m\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{W_1}L} =  \\color{red}{\\nabla_{Q_1}L}\\color{blue}X^T  \\rightarrow n_1 \\times d\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\color{red}{\\nabla_{B_1}L} =  \\sum_{m}\\color{red}{\\nabla_{Q_1}L}  \\rightarrow n_1 \\times 1\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "##### Update the parameters:\n",
    "\n",
    "For simplicity we again stick with vanilla gradient descent:\n",
    "\n",
    "\\begin{equation}\n",
    "    W_1 = W_1 - \\alpha\\color{red}{\\nabla_{W_1}L}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    B_1 = B_1 - \\alpha\\color{red}{\\nabla_{B_1}L}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    W_2 = W_2 - \\alpha\\color{red}{\\nabla_{W_2}L}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    B_2 = B_2 - \\alpha\\color{red}{\\nabla_{B_2}L}\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets code a two layer neural network!\n",
    "\n",
    "As a toy example let us train a two layer neural network to overfit on the following data:\n",
    "\n",
    "* $X \\rightarrow \\hat{Z}$\n",
    "* [0,0,1] = 1\n",
    "* [0,1,0] = 2\n",
    "* [0,1,1] = 3\n",
    "* [1,0,0] = 4\n",
    "* [1,0,1] = 5\n",
    "\n",
    "In this case we can see that $d=3$ and $m=5$. Also, notice that $n_2$ must be equal to $1$ in order to match the dimension of the desired outputs $\\hat{z}$.\n",
    "\n",
    "The only size we can select in this case is $n_1$. As an initial example let us choose $n_1=10$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                 # Standar python module for matrix and vector operations.\n",
    "\n",
    "#======================= DEFINE THE DATA ============================\n",
    "X = np.array([[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,0,1]]).T    # Input examples. Shape: (d x m)\n",
    "Z_hat = np.array([[1],[2],[3],[4],[5]]).T                    # Desired outputs. Shape: (n2 x m)\n",
    "\n",
    "#======================= DEFINE THE SIZES ============================\n",
    "d = 3                              # Dimension of input examples.\n",
    "m = 5                              # Number of input examples (batch_size).\n",
    "n1 = 10                            # Number of neurons in layer 1.\n",
    "n2 = 1                             # Number of neurons in layer 2.\n",
    "alpha = 0.1                        # Learning rate for gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the dimensions of the network, we also have to define the activation functions to use in each of the layers. For this example lets use a linear activation function for the output (this is required to achieve the scope of the output data) and hyperbolic tangents for layer 1 (this functions will allow us to introduce non-linearities into the neural network).\n",
    "\n",
    "To define the activation functions in a code-efficient way we have to consider the fact that the network will be propagating information in two ways: the forward way and the backward way. Therefore, the activation functions must provide a way to operate in both directions.\n",
    "\n",
    "**Let us ilustrate this with the hyperbolic tangent function:**\n",
    "\n",
    "In the forward pass, the $tanh$ function has to behave as usual:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = tanh(x)\n",
    "\\end{equation}\n",
    "\n",
    "However, in the backward pass the activation function must provide the gradient of its output w.r.t its input:\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x) = \\frac{df(x)}{dx} = 1 - tanh^2(x) \n",
    "\\end{equation}\n",
    "\n",
    "which is equal to:\n",
    "\\begin{equation}\n",
    "    f'(x) = 1 - (f(x))^2\n",
    "\\end{equation}\n",
    "\n",
    "To achieve this behavior in python we can use a boolean flag to tell the function how to operate. \n",
    "\n",
    "**For instance, for the $tanh$ function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x,deriv=False):            # The deriv flag allows us to run it backwards.\n",
    "    if(not deriv):\n",
    "        return np.tanh(x)           # In this case x is the forward input.\n",
    "    else:\n",
    "        return 1.0 - x**2           # In this case x is the forward output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact same idea applies for the linear function:\n",
    "\n",
    "\\begin{equation}\n",
    "    g(x) = x\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "\\begin{equation}\n",
    "    g'(x) = 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x,deriv=False):          # The deriv flag allows us to run it backwards.\n",
    "    if(not deriv):\n",
    "        return x                    # In this case x is the forward input.\n",
    "    else:\n",
    "        return 1.0                  # In this case the derivative is 1 regardless of the forward output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the neural network\n",
    "\n",
    "Once we have defined our dimensions and activation functions we can code the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0    Loss: 12.318547304958344\n",
      "Iteration: 100    Loss: 0.004690798719254695\n",
      "Iteration: 200    Loss: 0.0002006540557154712\n",
      "Iteration: 300    Loss: 3.41388537736427e-05\n",
      "Iteration: 400    Loss: 5.983661146724436e-06\n",
      "Iteration: 500    Loss: 1.0529712515311658e-06\n",
      "Iteration: 600    Loss: 1.8555981061054548e-07\n",
      "Iteration: 700    Loss: 3.271848730518615e-08\n",
      "Iteration: 800    Loss: 5.770334397200067e-09\n",
      "Iteration: 900    Loss: 1.0177697821188158e-09\n",
      "Iteration: 1000    Loss: 1.7952096075800864e-10\n",
      "__________________________\n",
      "Desired output:\n",
      "[1 2 3 4 5]\n",
      "Predicted output:\n",
      "[0.99999969 1.99998063 3.00002095 4.0000203  4.9999768 ]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)               # Fix random seed for reproducibility.\n",
    "\n",
    "#======================= INITIALIZE WEIGHTS ============================\n",
    "W1 = 2.0*np.random.random((n1,d)) - 1.0     # Uniformly distributed (-1,1). Shape: (n1 x d)\n",
    "B1 = 2.0*np.random.random((n1,1)) - 1.0     # Uniformly distributed (-1,1). Shape: (n1 x 1)\n",
    "W2 = 2.0*np.random.random((n2,n1)) - 1.0    # Uniformly distributed (-1,1). Shape: (n2 x n1)\n",
    "B2 = 2.0*np.random.random((n2,1)) - 1.0     # Uniformly distributed (-1,1). Shape: (n2 x 1)\n",
    "\n",
    "\n",
    "#======================= TRAIN THE NETWORK ============================\n",
    "max_iterations = 1000                        # Number of iterations to train.\n",
    "for iteration in range(max_iterations+1):    # Loop over the iterations.\n",
    "    \n",
    "    # FORWARD PASS\n",
    "    Y = tanh(np.dot(W1,X)+B1)                # Layer 1 output. Shape: (n1 x m)\n",
    "    Z = linear(np.dot(W2,Y)+B2)              # Layer 2 output. Shape: (n2 x m)\n",
    "    \n",
    "    # COMPUTE LOSS\n",
    "    L = np.square(Z_hat-Z)/(2.0*m)            # Loss. Shape: (n2 x m)\n",
    "    \n",
    "    # BACKWARD PASS\n",
    "    grad_Z = -(Z_hat-Z)/m                              # Gradient of L w.r.t Z.                 Shape: (n2 x m)\n",
    "    grad_Q2 = linear(Z,deriv=True)*grad_Z              # Intermeadiate gradient inside layer 2. Shape: (n2 x m)\n",
    "    grad_Y = np.dot(W2.T,grad_Q2)                      # Gradient of L w.r.t Y.                 Shape: (n1 x m)\n",
    "    grad_W2 = np.dot(grad_Q2,Y.T)                      # Gradient of L w.r.t W2.                Shape: (n2 x n1)\n",
    "    grad_B2 = np.sum(grad_Q2,axis=1).reshape(n2,1)     # Gradient of L w.r.t B2.                Shape: (n2 x 1)\n",
    "    \n",
    "    grad_Q1 = tanh(Y,deriv=True)*grad_Y                # Intermeadiate gradient inside layer 1. Shape: (n1 x m)\n",
    "    grad_X = np.dot(W1.T,grad_Q1)                      # Gradient of L w.r.t X.                 Shape: (d x m)\n",
    "    grad_W1 = np.dot(grad_Q1,X.T)                      # Gradient of L w.r.t W1.                Shape: (n1 x d)\n",
    "    grad_B1 = np.sum(grad_Q1,axis=1).reshape(n1,1)     # Gradient of L w.r.t B1.                Shape: (n1 x 1)\n",
    "    \n",
    "    # UPDATE WEIGHTS\n",
    "    W1 = W1 - alpha*grad_W1                   # Gradient descent on W1. Shape: (n1 x d)\n",
    "    B1 = B1 - alpha*grad_B1                   # Gradient descent on B1. Shape: (n1 x 1)\n",
    "    W2 = W2 - alpha*grad_W2                   # Gradient descent on W2. Shape: (n2 x 1)\n",
    "    B2 = B2 - alpha*grad_B2                   # Gradient descent on B2. Shape: (n2 x 1)\n",
    "    \n",
    "    if(iteration%(max_iterations/10))==0:\n",
    "        print('Iteration:',iteration,'   Loss:',L.sum()) # Print the training log.\n",
    "\n",
    "# ============================== TEST THE NETWORK =================================\n",
    "Y = tanh(np.dot(W1,X)+B1)\n",
    "Z = linear(np.dot(W2,Y)+B2)\n",
    "print('__________________________')\n",
    "print('Desired output:')\n",
    "print(Z_hat[0])\n",
    "print('Predicted output:')\n",
    "print(np.round(Z[0],8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The recipe to train a neural network\n",
    "\n",
    "In summary the process of training a neural network requires four steps:\n",
    "* **1.** Propagate input data through the network (forward pass).\n",
    "* **2.** Compute the loss (the quantity we want to minimize).\n",
    "* **3.** Backpropagate the error to the network and compute the desired gradients (backward pass).\n",
    "* **4.** Update the network parameters with an optimizer (i.e. gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, lets dive into Double Deep Q-learning\n",
    "\n",
    "As we saw in Part 1 of this tutorial, tabular Q-learning seeks to learn a table with $|S|$ rows and $|A|$ columns. In this table each entry is the Q-value of the corresponding state-action pair. In this tabular form the Q-learning update is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    Q(s_t,a_t) = Q(s_t,a_t) + \\alpha[r_{t+1} + \\gamma \\max_{\\hat{a}}Q(s_{t+1},\\hat{a}) - Q(s_t,a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "We saw that it is straighforward to implement this in code and that we can achieve optimal results. However, the tabular form fails to work when we deal with continuous or very big state spaces (i.e. we wont be able to build the required table).\n",
    "\n",
    "In order to solve this problem we can approximate the function $Q(s,a)$ with a neural network $Q(s,a|\\theta)$ with parameters $\\theta$. To do so, we need to re-define the update rule in the form of a loss function.\n",
    "\n",
    "As in tabular Q-learning, the quantity that we want to minimize is the Bellman Error:\n",
    "\n",
    "\\begin{equation}\n",
    "    BE = r_{t+1} + \\gamma \\max_{\\hat{a}}Q(s_{t+1},\\hat{a}|\\theta) - Q(s_t,a_t|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Hence, we can define the squared Bellman error as our loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{2}\\bigg(r_{t+1} + \\gamma \\max_{\\hat{a}}Q(s_{t+1},\\hat{a}|\\theta) - Q(s_t,a_t|\\theta)\\bigg)^2\n",
    "\\end{equation}\n",
    "\n",
    "We can see that this loss has the exact same shape as the loss function used in our vanilla neural network tutorial:\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{2}\\big(\\hat{z} - z\\big)^2\n",
    "\\end{equation}\n",
    "\n",
    "where the target $\\hat{z}$ (desired output) is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{z} = r_{t+1} + \\gamma \\max_{\\hat{a}}Q(s_{t+1},\\hat{a}|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "and the network's output $z$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    z = Q(s_t,a_t|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "The main difference between the previous supervised learning approach of our vanilla neural network, and the reinforcement learning approach used in this case, is that in the first one we knew the correct desired outputs $\\hat{z}$ (we had a list of data with the desired outputs given the inputs), while in the second one, the desired output $\\hat{z}$, for the state-action pair $(s_t, a_t)$, is a moving target based on the received inmediate reward, and an estimate of the Q-value for the next state $s_{t+1}$.\n",
    "\n",
    "### The double part of the algorithm\n",
    "\n",
    "The fact that we have a moving target is an undesired property for the stability of the neural network training process. To stabilize the training process, we can use two different neural networks: one to predict $z$ and one to predict the Q-value inside the target $\\hat{z}$. The use of these two distinct neural networks is what gives rise to the name: **Double Deep Q-learning**.\n",
    "\n",
    "The two networks:\n",
    "\n",
    "\\begin{equation}\n",
    "    z = Q(s_t,a_t|\\theta)\n",
    "\\end{equation}\n",
    " and \n",
    "\\begin{equation}\n",
    "    \\hat{z} = r_{t+1} + \\gamma \\max_{\\hat{a}}\\hat{Q}(s_{t+1},\\hat{a}|\\hat{\\theta})\n",
    "\\end{equation}\n",
    "\n",
    "are equal at the begining of the training process (i.e. $\\theta=\\hat{\\theta}$). However, during training, $\\theta$ is updated via backpropagation to minimize the bellman error and $\\hat{\\theta}$ is updated with a soft update rule like:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\theta} \\leftarrow \\tau\\theta + (1-\\tau)\\hat{\\theta}\n",
    "\\end{equation}\n",
    "\n",
    "### The use of past experiences\n",
    "\n",
    "In addition to the use of two neural networks, it is also a common practice to use a replay memory buffer to provide data to the networks while training. This allows for a more efficient use of data and a more stable training process. The use of replay memory works as follows:\n",
    "\n",
    "* During training the agent collects the experiences that it observes from its interaction with the environment and saves them in a memory buffer. The experiences that it collects are tuples of the form: $(s_t,a_t,s_{t+1},r_{t+1},done)$. Here, $done$ is a boolean flag that indicates if the state $s_{t+1}$ was a terminal state (True) or not (False).\n",
    "* Every training iteration the agent samples a random batch of experiences from the memory and performs a neural network training procedure on that batch.\n",
    "* Because the memory buffer has a maximum capacity (we dont have infinite memory in our computers), once the buffer is full it will drop the oldest experiences and replace them with the new incoming ones. \n",
    "\n",
    "\n",
    "# The Double Deep Q-learning algorithm\n",
    "The Double Deep Q-learning algorithm can be summarized as follows:\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jpmartinez10/RL-tutorial/master/numpy-based/imgs/DQN.png\">\n",
    "\n",
    "\n",
    "To understand better the Double Deep Q-learning algorithm lets use it to solve the grid_world of Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The agent class\n",
    "\n",
    "In order to save some time and focus on the fundamentals of Double Deep Q-learning, we have already coded an agent class in python.\n",
    "\n",
    "The agent class initializes a DQN agent with two neural networks: the main network $Q(s,a|\\theta)$ and the target network $\\hat{Q}(s,a|\\hat{\\theta})$. Both networks are initialized with the same weights and the weights are drawn from a Xavier initialization procedure (not relevant for now). \n",
    "\n",
    "The agent class has the following functions already implemented:\n",
    "\n",
    "* greedy(): takes an state as input and returns an action being greedy with the current Q-values.\n",
    "* epsilon_greedy(): takes an state as input and returns a random action with epsilon probability or a greedy action with (1-epsilon) probability.\n",
    "* epsilon_decay(): exponentially decays the exploration parameter epsilon (i.e. epsilon = decay$*$epsilon with decay<1)\n",
    "* forward_pass(): performs the forward pass on the network $Q(s,a|\\phi)$ given some parameters $\\phi$.\n",
    "* backward_pass(): performes the backpropagation algorithm and updates the weights $\\theta$.\n",
    "* update_target_weights(): performes the soft update on the target weights $\\hat{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the required python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                      # Standar libary for matrix/vector operations.\n",
    "import matplotlib.pyplot as plt                         # Python library to plot and render the environment.\n",
    "from envs.environments import grid_world                # The grid_world environment.\n",
    "from agents.Qlearning import double_deep_Qlearning      # A pre-made double_deep_Qlearning agent.\n",
    "from agents.Qlearning import ReplayBuffer               # A pre-made code for the replay buffer (store and sample data).\n",
    "from utilities import extract_policy, plot_policies_DQN # Pre-made functions to extract and plot policies.\n",
    "from utilities import plot_trayectory                   # Pre-made function to plot trayectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Deep Q-learning algorithm (main code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_double_Qlearning(env,agent,buffer,max_episodes,max_steps,exp_decay,start=False,seed=1234):\n",
    "    np.random.seed(seed)                                         # Fix random seed for reproducibility.\n",
    "    \n",
    "    agent.reset(fix_seed=True,seed=seed)                         # Initialize NN weights and target NN weights.\n",
    "    \n",
    "    initial_policy = extract_policy(env,agent)                   # Extract the initial policy of the agent.\n",
    "    \n",
    "    batch_size = 32                                              # The number of transitions to sample from the buffer. \n",
    "    min_memory = 1000                                            # The minimum memory to accumulate before training.\n",
    "    \n",
    "    for episode in range(max_episodes+1):                        # Loop over the episodes.\n",
    "        state = env.reset(start,True)                            # Reset the environment and initialize state s_t\n",
    "        step = 1                                                 # Step counter starts at 1.\n",
    "        done = False                                             # Boolean flag for terminal states.  \n",
    "        \n",
    "        while(not done and step <= max_steps):                   # Loop over the steps of episode.\n",
    "            action = agent.epsilon_greedy(state)                 # Choose action with epsilon-greedy policy.\n",
    "            next_state, reward, done = env.step(action,True)     # Take action and observe next_state s_(t+1) and reward.\n",
    "            buffer.add(state, action, next_state, reward, done)  # Add experience to the memory buffer.\n",
    "           \n",
    "            if(buffer.size() > min_memory):                      # If the current accumulated memory is > than minimum.\n",
    "                batch = buffer.sample_batch(batch_size)          # Sample a random batch of memory.\n",
    "                loss = agent.train(batch)                        # Train the agent on the sampled batch.\n",
    "                agent.update_target_weights()                    # Update the target network.\n",
    "            \n",
    "            state = next_state                                   # Update the current state.\n",
    "            step += 1                                            # Increase the step counter by 1.\n",
    "\n",
    "        agent.epsilon_decay(rate=exp_decay,min=0.1)             # Reduces the exploration parameter epsilon.\n",
    "\n",
    "        if(episode%(max_episodes/10)==0):                       # This prints some information of the training process.\n",
    "            print('Episode: ',episode,' Steps: ',step,\n",
    "                  ' Exploration: ',np.round(agent._epsilon,2))\n",
    "            \n",
    "    final_policy = extract_policy(env,agent)                    # Extract the final policy of the agent.\n",
    "    \n",
    "    return [initial_policy, final_policy]                       # Returns the initial and final policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the agent.train() function\n",
    "\n",
    "In order to understand Double Deep Q-learning, we are going to implement the train function of our deep_double_Qlearning agent.\n",
    "\n",
    "This function performs the update of the network $Q(s,a|\\theta)$ using the backpropagation algorithm to minimize the bellman error:\n",
    "\n",
    "\\begin{equation}\n",
    "    BE = r_{t+1} + \\gamma \\max_{\\hat{a}}\\hat{Q}(s_{t+1},\\hat{a}|\\hat{\\theta}) - Q(s_t,a_t|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "To implement only this function we are going to create an instance of the deep_double_Qlearning class and override the train method. But first, let us consider a quick trick for efficiency.\n",
    "\n",
    "### A quick trick for efficiency\n",
    "\n",
    "The most straighforward way to think about the neural network for the Deep Q-learning agent is the following:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jpmartinez10/RL-tutorial/master/numpy-based/imgs/DQN_st1.png\">\n",
    "\n",
    "We want to predict the value $Q(s_t,a_t)$, hence we take as inputs $s_t$ and $a_t$ and we predict one output which we will interpret as $Q(s_t,a_t)$. \n",
    "\n",
    "This first approach is correct from the theoretical point of view but it is inefficient from the practical one. Consider for instance an agent with $|A|$ actions. If we were to compute the maximization over the actions on the target network to minimize the bellman error:\n",
    "\n",
    "\\begin{equation}\n",
    "    BE = r_{t+1} + \\gamma \\color{red}{\\max_{\\hat{a}}\\hat{Q}(s_{t+1},\\hat{a}|\\hat{\\theta})}\\color{black}{- Q(s_t,a_t|\\theta)}\n",
    "\\end{equation}\n",
    "\n",
    "we would have to compute $|A|$ forward passes on the target network for every single computation of the loss in our training procedure. This is very inefficient as $|A|$ grows.\n",
    "\n",
    "A better way is to define the neural network with a less straighforward yet more efficient implementation: \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jpmartinez10/RL-tutorial/master/numpy-based/imgs/DQN_st2.png\">\n",
    "\n",
    "In this second approach the network takes as input only the state $s_t$, and it computes the Q-values for all the $|A|$ actions. This way will allow us to directly compute $\\max_{\\hat{a}}\\hat{Q}(s_{t+1},\\hat{a}|\\hat{\\theta})$ with one single forward pass of the network (instead of $|A|$ forward passes).\n",
    "\n",
    "The change with this strategy is that we will have to organize the training targets of every iteration changing only the output corresponding to the taken action $a_t$. \n",
    "\n",
    "**Lets see this in code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class our_double_deep_Qlearning(double_deep_Qlearning):\n",
    "    \n",
    "    def train(self,batch):                                   # Takes a batch of transitions as input.\n",
    "        for j in range(len(batch)):                          # For every transition j in the batch.\n",
    "            \n",
    "            state,action,next_state,reward,done = batch[j]   # Unpack the batch data.\n",
    "            \n",
    "            factor = 1.0 - np.array(done).astype(float)      # 0 or 1 depending on done.\n",
    "    \n",
    "            Qmax = np.amax(self.forward_pass(next_state,self._target_weights)[-1]) # Maximization over actions.\n",
    "                \n",
    "            target = reward + self._gamma*Qmax*factor        # Target.\n",
    "                \n",
    "\n",
    "\n",
    "#===========================================COMPUTATIONAL TRICK FOR EFFICIENCY======================================\n",
    "            outs = self.forward_pass(state)                  # Current predictions of the network.\n",
    "            z_hat = outs[-1].copy()                          # Select the one output to change.\n",
    "            z_hat[action] = target                           # Change the one output in the targets.\n",
    "            \n",
    "            if(j==0):                                        # For first trainsition in the batch.\n",
    "                layers = outs                                #\n",
    "                z_hats = z_hat                               #\n",
    "            else:                                            # For other transitions in the batch.\n",
    "                z_hats = np.hstack((z_hats,z_hat))             # Organize information for backward pass.\n",
    "                for i in range(self._n_layers+1):              # Organize information for backward pass.\n",
    "                    layers[i] = np.hstack((layers[i],outs[i])) # Organize information for backward pass.\n",
    "\n",
    "#===============================================END OF COMPUTATIONAL TRICK=========================================\n",
    "        \n",
    "        BE = self.backward_pass(z_hats,layers)         # Perform backward pass and update weights.\n",
    "        return BE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The reward function\n",
    "For the experiments of this part we will only use the second reward function of part 1 (which is the default reward function of the grid_world environment so we dont have to create a subclass as before).\n",
    "\n",
    "#### Reward \n",
    "\\begin{equation}\n",
    "    R(s) = \n",
    "        \\begin{cases}\n",
    "        +1, \\text{    if the agent is at the goal cell.}\\\\\n",
    "        -1, \\text{    otherwise.}\n",
    "        \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "## State representation for the Neural Network\n",
    "\n",
    "Recall that in the Part 1 of this tutorial the state representation of the environment was the number of the cell where the agent is located (i.e 0,1,2...63). This representation is great for the tabular form but it will fail with the neural network. A more convenient state representation for the Neural Network is the grid-coordiantes of the location of the agent (i.e [x,y]). We can use this representation simply by bypassing an aditional \"True\" argument to the env.reset() and env.step() functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets run some experiments!\n",
    "\n",
    "In order to explore the Double Deep Q-learning algorithm lets run some experiments. For each experiment, set the parameters of the **Train the agent** cell as shown and then try to answer the question. After training, run the **Test the agent** cell to see the behavior learned by the agent.\n",
    "\n",
    "#### Experiment 1\n",
    "* max_eps = 500\n",
    "* max_steps = 200\n",
    "* exp_decay = 0.99\n",
    "* random = False\n",
    "\n",
    "**Did the agent learn to solve the environment? Why not?**\n",
    "\n",
    "#### Experiment 2\n",
    "* max_eps = 1000\n",
    "* max_steps = 200\n",
    "* exp_decay = 0.99\n",
    "* random = False\n",
    "\n",
    "**Can the agent solve the environment now? Why not?**\n",
    "\n",
    "#### Experiment 3\n",
    "* max_eps = 1000\n",
    "* max_steps = 200\n",
    "* exp_decay = 0.999\n",
    "* random = False\n",
    "\n",
    "**Can the agent solve the environment from every possible state? Is the final policy optimal?**\n",
    "\n",
    "#### Experiment 4\n",
    "* max_eps = 1000\n",
    "* max_steps = 200\n",
    "* exp_decay = 0.999\n",
    "* random = True\n",
    "\n",
    "**Can the agent solve the environment from every possible state? Why not?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  Steps:  201  Exploration:  0.99\n",
      "Episode:  50  Steps:  201  Exploration:  0.6\n",
      "Episode:  100  Steps:  32  Exploration:  0.36\n",
      "Episode:  150  Steps:  23  Exploration:  0.22\n",
      "Episode:  200  Steps:  201  Exploration:  0.13\n",
      "Episode:  250  Steps:  18  Exploration:  0.1\n",
      "Episode:  300  Steps:  17  Exploration:  0.1\n",
      "Episode:  350  Steps:  20  Exploration:  0.1\n",
      "Episode:  400  Steps:  15  Exploration:  0.1\n",
      "Episode:  450  Steps:  32  Exploration:  0.1\n",
      "Episode:  500  Steps:  19  Exploration:  0.1\n",
      "______________________\n",
      "UP, RIGHT, DOWN, LEFT\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAAA/CAYAAABn/8O7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAAPRJREFUeJzt3TERAjEQQNG7G/CADFoE0NNgAXUIwQMSwMOhAIaG/IL36szsFn/SpMi8rusElaVegP8mQFICJCVAUgIkJUBSAiQlQFICJCVAUpt6gXeOy3noG+Hzchg5btqeHkPn3fbXofOW3X3+6tyvF4FPBEhKgKQESEqApARISoCkBEhKgKQESEqApARISoCkBEhKgKQESEqApARISoCkBEhKgKQESEqApARISoCkBEhKgKRm3zRQcgOSEiApAZISICkBkhIgKQGSEiApAZISICkBkhIgKQGSEiApAZISICkBkhIgKQGSEiApAZISICkBknoBGV4NeR+MBwcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "env = grid_world()                                                # Instance of the environment.\n",
    "max_eps = 500                                                     # Max number of episodes to train.        \n",
    "max_steps = 200                                                   # Max steps per episode.\n",
    "exp_decay = 0.99                                                  # Decay rate for the exploration epsilon.\n",
    "random = False                                                    # Flag to use random initial state.\n",
    "\n",
    "dims = [2,20,4]                                                   # Dimensions of the neural network of the agent.\n",
    "                                                                  # dims are given as: [input,layer1,layer2]\n",
    "                                                                  # input = 2 due to the state representation: [x,y]\n",
    "                                                                  # layer1 = 20 means we have 20 neurons in layer 1\n",
    "                                                                  # layer2 = 4 means we have 4 neurons in the output\n",
    "                                                                  #  layer, this is due to the computational trick.\n",
    "    \n",
    "agent = our_double_deep_Qlearning(dims,lr=0.01)                       # Creates a Double Deep Q-learning agent.\n",
    "\n",
    "buffer = ReplayBuffer()                                           # Creates a Replay buffer.\n",
    "\n",
    "policies = run_deep_double_Qlearning(env,agent,buffer,max_eps,max_steps,exp_decay,random) # Run the DDQL algorithm.\n",
    "\n",
    "plot_policies_DQN(policies)                       # Display the initial and final policies of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"299.75\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "env = grid_world()                             # A new instance of the environment (required for rendering).\n",
    "\n",
    "for episode in range(5):                       # Loop over some episodes.\n",
    "    state = env.reset(random,True)             # Reset the environment.\n",
    "    done = False                               # done flag starts as false.\n",
    "    step = 1                                   # step counter starts in 1.\n",
    "    \n",
    "    trayectory = [env.get_map()]               # Python list to save trayectory.\n",
    "    while (not done and step < 20):            # Looping until terminal or during max steps.\n",
    "        action = agent.greedy(state)           # Select action based on greedy policy.\n",
    "        state,_,done = env.step(action,True)   # Take selected action.\n",
    "        step += 1                              # Increase step counter by one.\n",
    "        env_map = env.render()                 # Render the environment.\n",
    "        trayectory.append(env_map)             # Saves the current map in the trayectory.\n",
    "        \n",
    "%matplotlib inline\n",
    "\n",
    "plot_trayectory(trayectory)                    # Plots trayectory of last episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "\n",
    "* Double Deep Q-learning is able to generalize information. Hence, it is able to learn the good (even optimal) policies in large state-space environments.\n",
    "* As we saw with experiments 3 and 4, more robust policies can be achieved with harder training scenarios that provide better generalization.\n",
    "* Double Deep Q-learning requires more episodes to train because it is learning a function, not memorizing a table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
